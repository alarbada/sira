# Arcsiris

Yet another local llm playground.

This one works by having a template file and a params configuration file. You "exec" the folder and the openai llm output is printed in a streaming fashion.

Put the api key inside the `~/.arcsiris` file.


TODO:

- default directory?
- init folder
- save conversation inside a file. When running arcsiris with something like a "continue" argument, it continues the conversation.
- add hability to comment parts of the prompt
