# Arcsiris

Yet another local llm playground.

This one works by having a template file and a params configuration file. You "exec" the folder and the openai llm output is printed in a streaming fashion.

FIXME: no!
Put the api key inside the `~/.arcsiris` file.


TODO:

- make tests pass
- add debug info
- parse openai token from a .toml file instead of what's there now
